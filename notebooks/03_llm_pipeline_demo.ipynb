{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Pipeline Demo for Wearable Insights\n",
    "\n",
    "This notebook demonstrates the end-to-end pipeline for generating insights from wearable data using LLMs. We'll use the processed data and prompt templates from previous notebooks to generate personalized insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add the src directory to the path so we can import our modules\n",
    "sys.path.append('../')\n",
    "from src.data_loader import DataLoader\n",
    "from src.feature_engineer import FeatureEngineer\n",
    "from src.insight_prompt_builder import InsightPromptBuilder\n",
    "from src.llm_engine import LLMEngine\n",
    "\n",
    "# Set up plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Processed Data\n",
    "\n",
    "First, let's load the processed data from the previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load combined features\n",
    "processed_dir = '../data/processed'\n",
    "with open(os.path.join(processed_dir, 'combined_features.json'), 'r') as f:\n",
    "    combined_features = json.load(f)\n",
    "\n",
    "# Load user goals\n",
    "with open(os.path.join(processed_dir, 'user_goals.json'), 'r') as f:\n",
    "    user_goals = json.load(f)\n",
    "\n",
    "print(f\"Loaded data for {len(combined_features)} days\")\n",
    "print(f\"User: {user_goals['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most recent day's data\n",
    "dates = list(combined_features.keys())\n",
    "dates.sort()\n",
    "latest_date = dates[-1]\n",
    "latest_features = combined_features[latest_date]\n",
    "\n",
    "print(f\"Latest data date: {latest_date}\")\n",
    "print(f\"Number of features: {len(latest_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Components\n",
    "\n",
    "Now, let's initialize all the components we need for the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize prompt builder\n",
    "prompt_builder = InsightPromptBuilder()\n",
    "\n",
    "# Initialize LLM engine\n",
    "llm_engine = LLMEngine()\n",
    "\n",
    "# Check if API key is set\n",
    "api_key_set = os.environ.get(\"OPENAI_API_KEY\") is not None\n",
    "if not api_key_set:\n",
    "    print(\"WARNING: OpenAI API key not set. Some cells will not generate actual insights.\")\n",
    "    print(\"To set the API key, run: export OPENAI_API_KEY='your-api-key'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. End-to-End Pipeline: Daily Insight Generation\n",
    "\n",
    "Let's implement a complete pipeline for generating daily insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_daily_insight(date, features, user_goals, tone=\"coach\"):\n",
    "    \"\"\"Generate a daily insight for the specified date.\"\"\"\n",
    "    # Extract user's primary fitness goal\n",
    "    fitness_goal = next((goal['goal'] for goal in user_goals['primary_goals'] if goal['area'] == 'fitness'), \n",
    "                        \"improving overall fitness\")\n",
    "    \n",
    "    # Build prompt\n",
    "    prompt = prompt_builder.build_prompt(\n",
    "        features,\n",
    "        tone=tone,\n",
    "        user_goal=fitness_goal,\n",
    "        time_range=f\"data from {date}\"\n",
    "    )\n",
    "    \n",
    "    # Generate insight\n",
    "    if api_key_set:\n",
    "        insight, metadata = llm_engine.generate_insight(prompt)\n",
    "    else:\n",
    "        # Simulate insight if API key not set\n",
    "        insight = \"[This is a simulated insight. Set OPENAI_API_KEY to generate real insights.]\"\n",
    "        metadata = {\"model\": \"simulation\", \"latency_seconds\": 0, \"total_tokens\": 0}\n",
    "    \n",
    "    return {\n",
    "        \"date\": date,\n",
    "        \"prompt\": prompt,\n",
    "        \"insight\": insight,\n",
    "        \"metadata\": metadata\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate insight for the latest day\n",
    "latest_insight = generate_daily_insight(latest_date, latest_features, user_goals)\n",
    "\n",
    "print(f\"=== DAILY INSIGHT FOR {latest_date} ===\\n\")\n",
    "print(latest_insight[\"insight\"])\n",
    "\n",
    "if api_key_set:\n",
    "    print(\"\\nMetadata:\")\n",
    "    print(f\"- Model: {latest_insight['metadata'].get('model')}\")\n",
    "    print(f\"- Tokens: {latest_insight['metadata'].get('total_tokens')}\")\n",
    "    print(f\"- Latency: {latest_insight['metadata'].get('latency_seconds'):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Multiple Insights\n",
    "\n",
    "Now, let's generate insights for all available days and different focus areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate insights for all days (limited to last 3 days to save API calls)\n",
    "days_to_process = min(3, len(dates))\n",
    "recent_dates = dates[-days_to_process:]\n",
    "\n",
    "daily_insights = []\n",
    "for date in recent_dates:\n",
    "    features = combined_features[date]\n",
    "    insight = generate_daily_insight(date, features, user_goals)\n",
    "    daily_insights.append(insight)\n",
    "    print(f\"Generated insight for {date}\")\n",
    "\n",
    "print(f\"\\nGenerated {len(daily_insights)} daily insights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate focused insights for different areas\n",
    "focus_areas = [\"sleep\", \"recovery\", \"activity\"]\n",
    "focused_insights = []\n",
    "\n",
    "for focus in focus_areas:\n",
    "    # Get relevant user goal for this focus area\n",
    "    goal = next((g['goal'] for g in user_goals['primary_goals'] if g['area'] == focus), None)\n",
    "    \n",
    "    # Build focused prompt\n",
    "    prompt = prompt_builder.build_focused_prompt(\n",
    "        latest_features,\n",
    "        focus_area=focus,\n",
    "        user_goal=goal\n",
    "    )\n",
    "    \n",
    "    # Generate insight\n",
    "    if api_key_set:\n",
    "        insight, metadata = llm_engine.generate_insight(prompt)\n",
    "    else:\n",
    "        # Simulate insight if API key not set\n",
    "        insight = f\"[This is a simulated {focus}-focused insight. Set OPENAI_API_KEY to generate real insights.]\"\n",
    "        metadata = {\"model\": \"simulation\", \"latency_seconds\": 0, \"total_tokens\": 0}\n",
    "    \n",
    "    focused_insights.append({\n",
    "        \"focus_area\": focus,\n",
    "        \"prompt\": prompt,\n",
    "        \"insight\": insight,\n",
    "        \"metadata\": metadata\n",
    "    })\n",
    "    \n",
    "    print(f\"Generated {focus}-focused insight\")\n",
    "\n",
    "print(f\"\\nGenerated {len(focused_insights)} focused insights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Display Generated Insights\n",
    "\n",
    "Let's display the generated insights in a readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display daily insights\n",
    "for insight in daily_insights:\n",
    "    print(f\"=== DAILY INSIGHT FOR {insight['date']} ===\\n\")\n",
    "    print(insight[\"insight\"])\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display focused insights\n",
    "for insight in focused_insights:\n",
    "    print(f\"=== {insight['focus_area'].upper()}-FOCUSED INSIGHT ===\\n\")\n",
    "    print(insight[\"insight\"])\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Generated Insights\n",
    "\n",
    "Let's save the generated insights for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create outputs directory if it doesn't exist\n",
    "outputs_dir = '../outputs'\n",
    "os.makedirs(outputs_dir, exist_ok=True)\n",
    "\n",
    "# Save daily insights\n",
    "daily_insights_path = os.path.join(outputs_dir, 'daily_insights.json')\n",
    "with open(daily_insights_path, 'w') as f:\n",
    "    # Convert dates to strings for JSON serialization\n",
    "    serializable_insights = []\n",
    "    for insight in daily_insights:\n",
    "        serializable_insight = insight.copy()\n",
    "        serializable_insight['metadata'] = {k: str(v) if isinstance(v, (datetime, timedelta)) else v \n",
    "                                          for k, v in insight['metadata'].items()}\n",
    "        serializable_insights.append(serializable_insight)\n",
    "    \n",
    "    json.dump(serializable_insights, f, indent=2)\n",
    "\n",
    "# Save focused insights\n",
    "focused_insights_path = os.path.join(outputs_dir, 'focused_insights.json')\n",
    "with open(focused_insights_path, 'w') as f:\n",
    "    # Convert dates to strings for JSON serialization\n",
    "    serializable_insights = []\n",
    "    for insight in focused_insights:\n",
    "        serializable_insight = insight.copy()\n",
    "        serializable_insight['metadata'] = {k: str(v) if isinstance(v, (datetime, timedelta)) else v \n",
    "                                          for k, v in insight['metadata'].items()}\n",
    "        serializable_insights.append(serializable_insight)\n",
    "    \n",
    "    json.dump(serializable_insights, f, indent=2)\n",
    "\n",
    "print(f\"Saved daily insights to {daily_insights_path}\")\n",
    "print(f\"Saved focused insights to {focused_insights_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Implement a Complete Insight Pipeline Function\n",
    "\n",
    "Let's create a reusable function that implements the complete insight generation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_insights_pipeline(data_dir, raw_data_files, user_goals_file, output_dir, \n",
    "                              generate_daily=True, generate_focused=True, \n",
    "                              focus_areas=None, days_to_process=1):\n",
    "    \"\"\"Complete pipeline for generating insights from raw wearable data.\"\"\"\n",
    "    # Initialize components\n",
    "    loader = DataLoader(data_dir=data_dir)\n",
    "    feature_eng = FeatureEngineer()\n",
    "    prompt_builder = InsightPromptBuilder()\n",
    "    llm_engine = LLMEngine()\n",
    "    \n",
    "    # Step 1: Load and process raw data\n",
    "    print(\"Step 1: Loading and processing raw data...\")\n",
    "    \n",
    "    # Load user goals\n",
    "    user_goals = loader.load_file(user_goals_file)\n",
    "    \n",
    "    # Process each raw data file\n",
    "    hrv_data = None\n",
    "    activity_sleep_data = None\n",
    "    \n",
    "    for file in raw_data_files:\n",
    "        data = loader.load_file(file)\n",
    "        \n",
    "        if file.endswith('.csv') and 'hrv' in file.lower():\n",
    "            hrv_data = data\n",
    "        elif file.endswith('.json') and 'activity' in file.lower():\n",
    "            activity_sleep_data = data\n",
    "    \n",
    "    if hrv_data is None or activity_sleep_data is None:\n",
    "        print(\"Error: Missing required data files\")\n",
    "        return None\n",
    "    \n",
    "    # Step 2: Extract features\n",
    "    print(\"Step 2: Extracting features...\")\n",
    "    \n",
    "    # Process HRV data\n",
    "    normalized_hrv = loader.normalize_data(hrv_data, 'hrv')\n",
    "    cleaned_hrv = loader.clean_data(normalized_hrv)\n",
    "    daily_hrv = loader.segment_by_day(cleaned_hrv)\n",
    "    \n",
    "    # Process activity data\n",
    "    activities_df = pd.DataFrame(activity_sleep_data['activities'])\n",
    "    activities_by_date = {}\n",
    "    for activity in activity_sleep_data['activities']:\n",
    "        date = activity['date']\n",
    "        if date not in activities_by_date:\n",
    "            activities_by_date[date] = []\n",
    "        activities_by_date[date].append(activity)\n",
    "    \n",
    "    activity_dfs_by_day = {}\n",
    "    for date, activities in activities_by_date.items():\n",
    "        activity_dfs_by_day[date] = pd.DataFrame(activities)\n",
    "    \n",
    "    # Process sleep data\n",
    "    sleep_df = pd.DataFrame(activity_sleep_data['sleep_sessions'])\n",
    "    sleep_by_date = {}\n",
    "    for sleep in activity_sleep_data['sleep_sessions']:\n",
    "        date = sleep['date']\n",
    "        if date not in sleep_by_date:\n",
    "            sleep_by_date[date] = []\n",
    "        sleep_by_date[date].append(sleep)\n",
    "    \n",
    "    sleep_dfs_by_day = {}\n",
    "    for date, sleeps in sleep_by_date.items():\n",
    "        sleep_dfs_by_day[date] = pd.DataFrame(sleeps)\n",
    "    \n",
    "    # Extract features for each day\n",
    "    combined_features_by_day = {}\n",
    "    \n",
    "    # Get all unique dates\n",
    "    all_dates = set(list(daily_hrv.keys()) + \n",
    "                    list(activity_dfs_by_day.keys()) + \n",
    "                    list(sleep_dfs_by_day.keys()))\n",
    "    \n",
    "    # Sort dates\n",
    "    all_dates = sorted(list(all_dates))\n",
    "    \n",
    "    # Limit to specified number of days\n",
    "    recent_dates = all_dates[-days_to_process:] if len(all_dates) > days_to_process else all_dates\n",
    "    \n",
    "    # Calculate training load\n",
    "    activity_dfs = list(activity_dfs_by_day.values())\n",
    "    training_load = feature_eng.calculate_training_load(activity_dfs)\n",
    "    \n",
    "    # Extract features for each day\n",
    "    for date in recent_dates:\n",
    "        hrv_features = feature_eng.extract_hrv_features(daily_hrv[date]) if date in daily_hrv else {}\n",
    "        activity_features = feature_eng.extract_activity_features(activity_dfs_by_day[date]) if date in activity_dfs_by_day else {}\n",
    "        sleep_features = feature_eng.extract_sleep_features(sleep_dfs_by_day[date]) if date in sleep_dfs_by_day else {}\n",
    "        \n",
    "        combined_features_by_day[date] = feature_eng.combine_features(\n",
    "            hrv_features, activity_features, sleep_features, training_load\n",
    "        )\n",
    "    \n",
    "    # Step 3: Generate insights\n",
    "    print(\"Step 3: Generating insights...\")\n",
    "    \n",
    "    results = {\n",
    "        \"daily_insights\": [],\n",
    "        \"focused_insights\": []\n",
    "    }\n",
    "    \n",
    "    # Generate daily insights\n",
    "    if generate_daily:\n",
    "        for date in recent_dates:\n",
    "            features = combined_features_by_day[date]\n",
    "            insight = generate_daily_insight(date, features, user_goals)\n",
    "            results[\"daily_insights\"].append(insight)\n",
    "            print(f\"Generated daily insight for {date}\")\n",
    "    \n",
    "    # Generate focused insights\n",
    "    if generate_focused:\n",
    "        focus_areas = focus_areas or [\"sleep\", \"recovery\", \"activity\"]\n",
    "        latest_date = recent_dates[-1]\n",
    "        latest_features = combined_features_by_day[latest_date]\n",
    "        \n",
    "        for focus in focus_areas:\n",
    "            # Get relevant user goal for this focus area\n",
    "            goal = next((g['goal'] for g in user_goals['primary_goals'] if g['area'] == focus), None)\n",
    "            \n",
    "            # Build focused prompt\n",
    "            prompt = prompt_builder.build_focused_prompt(\n",
    "                latest_features,\n",
    "                focus_area=focus,\n",
    "                user_goal=goal\n",
    "            )\n",
    "            \n",
    "            # Generate insight\n",
    "            if api_key_set:\n",
    "                insight, metadata = llm_engine.generate_insight(prompt)\n",
    "            else:\n",
    "                # Simulate insight if API key not set\n",
    "                insight = f\"[This is a simulated {focus}-focused insight. Set OPENAI_API_KEY to generate real insights.]\"\n",
    "                metadata = {\"model\": \"simulation\", \"latency_seconds\": 0, \"total_tokens\": 0}\n",
    "            \n",
    "            results[\"focused_insights\"].append({\n",
    "                \"focus_area\": focus,\n",
    "                \"prompt\": prompt,\n",
    "                \"insight\": insight,\n",
    "                \"metadata\": metadata\n",
    "            })\n",
    "            \n",
    "            print(f\"Generated {focus}-focused insight\")\n",
    "    \n",
    "    # Step 4: Save results\n",
    "    print(\"Step 4: Saving results...\")\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save daily insights\n",
    "    if generate_daily:\n",
    "        daily_insights_path = os.path.join(output_dir, 'daily_insights.json')\n",
    "        with open(daily_insights_path, 'w') as f:\n",
    "            # Convert dates to strings for JSON serialization\n",
    "            serializable_insights = []\n",
    "            for insight in results[\"daily_insights\"]:\n",
    "                serializable_insight = insight.copy()\n",
    "                serializable_insight['metadata'] = {k: str(v) if isinstance(v, (datetime, timedelta)) else v \n",
    "                                                  for k, v in insight['metadata'].items()}\n",
    "                serializable_insights.append(serializable_insight)\n",
    "            \n",
    "            json.dump(serializable_insights, f, indent=2)\n",
    "        \n",
    "        print(f\"Saved daily insights to {daily_insights_path}\")\n",
    "    \n",
    "    # Save focused insights\n",
    "    if generate_focused:\n",
    "        focused_insights_path = os.path.join(output_dir, 'focused_insights.json')\n",
    "        with open(focused_insights_path, 'w') as f:\n",
    "            # Convert dates to strings for JSON serialization\n",
    "            serializable_insights = []\n",
    "            for insight in results[\"focused_insights\"]:\n",
    "                serializable_insight = insight.copy()\n",
    "                serializable_insight['metadata'] = {k: str(v) if isinstance(v, (datetime, timedelta)) else v \n",
    "                                                  for k, v in insight['metadata'].items()}\n",
    "                serializable_insights.append(serializable_insight)\n",
    "            \n",
    "            json.dump(serializable_insights, f, indent=2)\n",
    "        \n",
    "        print(f\"Saved focused insights to {focused_insights_path}\")\n",
    "    \n",
    "    print(\"Pipeline completed successfully!\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the complete pipeline\n",
    "pipeline_results = generate_insights_pipeline(\n",
    "    data_dir='../data/raw',\n",
    "    raw_data_files=['sample_hrv_data.csv', 'sample_activity_sleep.json'],\n",
    "    user_goals_file='user_goals.json',\n",
    "    output_dir='../outputs',\n",
    "    days_to_process=2,\n",
    "    focus_areas=[\"sleep\", \"recovery\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've demonstrated the complete end-to-end pipeline for generating insights from wearable data using LLMs:\n",
    "\n",
    "1. **Data Loading**: We loaded the processed wearable data and user goals.\n",
    "\n",
    "2. **Insight Generation**: We generated daily insights for recent days and focused insights for specific areas like sleep and recovery.\n",
    "\n",
    "3. **Pipeline Implementation**: We implemented a reusable pipeline function that can process raw data files, extract features, generate insights, and save the results.\n",
    "\n",
    "This pipeline can be integrated into a production system to automatically generate personalized insights from wearable data on a daily basis. The insights can be delivered to users through various channels, such as a mobile app, email, or a web dashboard.\n",
    "\n",
    "In the next notebook, we'll explore how to implement an interactive agent that can respond to user queries about their wearable data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
